###########
# Dataset #
###########

data_root: 'data_files'
batch_size: 89
num_workers: 0
eval_batch_size: 100
eval_num_workers: 0

label_offset:
  mnist: 0
  svhn: 0
  usps: 0
  CLOFA: 0
  CLOFD: 0
  CLOFC: 0
  CLOFW: 0

#########
# Model #
#########

x_c: 1
x_h: 4096
x_w: 1
y_dim: 10
d_dim: 4

device: 'cuda'
testing_mode: True
model_name: 'ClOf'
recon_loss: 'bernouiiilli'

###########
# Summary #
###########

disable_profiler: True

add_time_to_log: True
print_times: False

eval_per_task:
eval_step: 2022
initial_evaluation: False
eval_in_task_change: False

summary_per_task:
summary_step: 2022
summarize_samples: True

training_loss_step: 10
seed: 1
##########
# Replay #
##########
enable_replay: True
replay_batch_size: 10
replay_ratio: 1
replay_loss_multiplier: 1
equal_loss_scale: False
scale_replay_loss_wrt_num_tasks: True
################
# DIVA configs #
################
model:
  px_use_diva_conv: False # DIVA ConvNet
  px_use_our_conv: True # ConvNet for BCE
  # O.W: FC is used
  use_discriminator_y: True # label adversarial discriminator
  use_discriminator_d: True # adversarial discriminator

  use_diva_pzd: True # DIVA's multi-layer pzd
  freeze_pzd: False
  # O.W: a single layer pzd is used

  use_diva_pzy: True # DIVA's multi-layer pzy
  freeze_pzy: False
  # O.W: a single layer pzy is used

  use_diva_qzd: True # O.W: FC is used
  use_diva_qzy: True # O.W: FC is used
  use_diva_qzx: True # O.W: FC is used

  use_diva_qd: True
  use_bayes_qd: False
  freeze_qd: False
  # O.W: same classifier as DIVA is returned
  # except that it's activation is Softmax.

  use_diva_qy: True
  use_bayes_qy: False
  freeze_qy: False
  # O.W: same classifier as DIVA is returned
  # except that it's activation is Softmax.

  recon_loss: 'MSE'

  # Training settings
  description: 'TwoTaskVae'
  no-cuda: False

  lr: 0.00002

  zd_dim: 64
  zx_dim: 0
  zy_dim: 64

  # Aux multipliers
  aux_loss_multiplier_y: 5000
  aux_loss_multiplier_d: 8000
  aux_loss_discriminator: 5000
  # Beta VAE part
  beta_d: 1
  beta_x: 1
  beta_y: 1

  warm_up: 0.3
#
# w, warm_up, N: 100
#  max_beta, MB: 1    help='max beta for warm-up')
#  min_beta, MB: 0    help='min beta for warm-up')
#
#  # Set seed
#  torch.manual_seed(args.seed)
#  torch.backends.cudnn.benchmark = False
#  np.random.seed(args.seed)
