###########
# Dataset #
###########

data_root: 'data_files'
batch_size: 16
num_workers: 6
sleep_batch_size: 50
sleep_num_workers: 2
eval_batch_size: 16
eval_num_workers: 6

label_offset:
  mnist: 0
  svhn: 0
  usps: 0

#########
# Model #
#########

x_c: 1
x_h: 28
x_w: 28
y_c: 10

device: 'cuda'

testing_mode: True

model_name: 'diva'
g: 'mlp_sharing_vae'
d: 'mlp_sharing_classifier'
disable_d: False
vae_nf_base: 64
vae_nf_ext: 16
cls_nf_base: 64
cls_nf_ext: 16
z_dim: 16
z_samples: 16

recon_loss: 'gaussian'
x_log_var_param: 0
learn_x_log_var: false
classifier_chill: 0.01


#########
# DPMoE #
#########

log_alpha: -400
stm_capacity: 500
stm_erase_period: 0
sleep_step_g: 8000
sleep_step_d: 2000
sleep_summary_step: 500
sleep_val_size: 0
update_min_usage: 0.1


#########
# Train #
#########

implicit_lr_decay: false
weight_decay: 0.00001

optimizer_g:
  type: Adam
  options:
    lr: 0.0004

lr_scheduler_g:
  type: MultiStepLR
  options:
    milestones: [1]
    gamma: 1.0

optimizer_d:
  type: Adam
  options:
    lr: 0.0001

lr_scheduler_d:
  type: MultiStepLR
  options:
    milestones: [1]
    gamma: 1.0

clip_grad:
  type: value
  options:
    clip_value: 0.5


########
# Eval #
########

eval_d: True
eval_g: False
eval_t: False

###########
# Summary #
###########

summary_step: 250
eval_step: 250
summarize_samples: False




################
# DIVA configs #
################
diva:
  # Training settings
  description: 'TwoTaskVae'
  no-cuda: False
  seed: 0
  batch-size: 100
  epochs: 500
  lr: 0.001
  num-supervised: 1000


  # Choose domains
  list_train_domains: ['0','15', '30','45','60','75']
  list_test_domains: '75'

  # Model
  d_dim: 5
  x_dim: 784
  y_dim: 10
  zd_dim: 64
  zx_dim: 64
  zy_dim: 64

  # Aux multipliers
  aux_loss_multiplier_y: 3500
  aux_loss_multiplier_d: 2000

  # Beta VAE part
  beta_d: 1
  beta_x: 1
  beta_y: 1
#
#  w: 100
#  warmup: 100
#
#  parser.add_argument('--beta_d', type=float, default=1.,
#                      help='multiplier for KL d')
#  parser.add_argument('--beta_x', type=float, default=1.,
#                      help='multiplier for KL x')
#  parser.add_argument('--beta_y', type=float, default=1.,
#                      help='multiplier for KL y')
#
#  parser.add_argument('-w', '--warmup', type=int, default=100, metavar='N',
#                      help='number of epochs for warm-up. Set to 0 to turn warmup off.')
#  parser.add_argument('--max_beta', type=float, default=1., metavar='MB',
#                      help='max beta for warm-up')
#  parser.add_argument('--min_beta', type=float, default=0.0, metavar='MB',
#                      help='min beta for warm-up')
#
#  parser.add_argument('--outpath', type=str, default='./',
#                      help='where to save')
#
#  args = parser.parse_args()
#  args.cuda = not args.no_cuda and torch.cuda.is_available()
#  device = torch.device("cuda" if args.cuda else "cpu")
#  kwargs = {'num_workers': 1, 'pin_memory': False} if args.cuda else {}
#
#  # Model name
#  args.list_test_domain = [args.list_test_domain]
#  print(args.outpath)
#  model_name = args.outpath + 'test_domain_' + str(args.list_test_domain[0]) + '_diva_seed_' + str(
#      args.seed)
#  print(model_name)
#
#  # Choose training domains
#  all_training_domains = ['0', '15', '30', '45', '60', '75']
#  all_training_domains.remove(args.list_test_domain[0])
#  args.list_train_domains = all_training_domains
#
#  print(args.list_test_domain, args.list_train_domains)
#
#  # Set seed
#  torch.manual_seed(args.seed)
#  torch.backends.cudnn.benchmark = False
#  np.random.seed(args.seed)
#
#  # Load supervised training
#  train_loader = data_utils.DataLoader(
#      MnistRotated(args.list_train_domains, args.list_test_domain, args.num_supervised, args.seed, './../dataset/',
#                   train=True),
#      batch_size=args.batch_size,
#      shuffle=True, **kwargs)
