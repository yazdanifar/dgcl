###########
# Dataset #
###########

data_root: 'data_files'
batch_size: 100
num_workers: 1
eval_batch_size: 1024
eval_num_workers: 1

label_offset:
  mnist_small: 0
  mnist: 0
  svhn: 0
  usps: 0

#########
# Model #
#########

x_c: 1
x_h: 28
x_w: 28
y_dim: 10
d_dim: 6

device: 'cuda'
disable_profiler: True
testing_mode: True
model_name: 'OurDIVAtoOurDiva'
recon_loss: 'bernouiiilli'

###########
# Summary #
###########

add_time_to_log: True
print_times: False

eval_per_task: 20
eval_step:
initial_evaluation: False
eval_in_task_change: False

summary_per_task: 20
summary_step:
summarize_samples: True

training_loss_step: 10

##########
# Replay #
##########
replay_batch_size: 256
replay_ratio: 0
replay_loss_multiplier: 1
equal_loss_scale: False
scale_replay_loss_wrt_num_tasks: True
################
# DIVA configs #
################
model:
  px_use_diva_conv: True # DIVA ConvNet
  px_use_our_conv: False # ConvNet for BCE
  # O.W: FC is used
  use_discriminator_y: False # label adversarial discriminator
  use_discriminator_d: False # adversarial discriminator

  use_diva_pzd: True # DIVA's multi-layer pzd
  freeze_pzd: False
  # O.W: a single layer pzd is used

  use_diva_pzy: True # DIVA's multi-layer pzy
  freeze_pzy: False
  # O.W: a single layer pzy is used

  use_diva_qzd: True # O.W: FC is used
  use_diva_qzy: True # O.W: FC is used
  use_diva_qzx: True # O.W: FC is used

  use_diva_qd: True
  use_bayes_qd: False
  freeze_qd: False
  # O.W: same classifier as DIVA is returned
  # except that it's activation is Softmax.

  use_diva_qy: True
  use_bayes_qy: False
  freeze_qy: False
  # O.W: same classifier as DIVA is returned
  # except that it's activation is Softmax.

  recon_loss: 'BCE'
  # Training settings
  description: 'TwoTaskVae'
  no-cuda: False
  seed: 0
  lr: 0.001

  zd_dim: 64
  zx_dim: 64
  zy_dim: 64

  # Aux multipliers
  aux_loss_multiplier_y: 4200
  aux_loss_multiplier_d: 2000
  aux_loss_discriminator: 0
  # Beta VAE part
  beta_d: 1
  beta_x: 1
  beta_y: 1

  warm_up: 100
#
# w, warm_up, N: 100
#  max_beta, MB: 1    help='max beta for warm-up')
#  min_beta, MB: 0    help='min beta for warm-up')
